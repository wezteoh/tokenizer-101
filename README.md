# tokenizer-101
This repo is developed along my deep-dive into the details of tokenization for LLM development, following the amazing tutorial by Andrej Karpathy [[Link](https://www.youtube.com/watch?v=zduSFxRajkE)]. It heavily references his example repo [[Link](https://github.com/karpathy/minbpe)].

What I plan to do next:
- Efficiency benchmark across different major tokenizer: sentencepiece, tiktoken, my own trained tokenizer on contexts of different nature
- Explore implementations to optimize the tokenization process for huge training data